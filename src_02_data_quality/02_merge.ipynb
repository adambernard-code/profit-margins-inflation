{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "648778e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded using lazy evaluation\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import os\n",
    "\n",
    "# Load the data using lazy evaluation for better performance\n",
    "main_path = os.path.join(\"..\", \"data\", \"source_cleaned\", \"magnusweb_panel_imputed.parquet\")\n",
    "#main_path = os.path.join(\"..\", \"data\", \"source_cleaned\", \"magnusweb_panel_hq.parquet\")\n",
    "\n",
    "# Output path for the final merged dataset\n",
    "output_path = os.path.join(\"..\", \"data\", \"data_ready\", \"merged_panel_imputed.parquet\")\n",
    "#output_path = os.path.join(\"..\", \"data\", \"data_ready\", \"merged_panel_hq.parquet\")\n",
    "\n",
    "# Using scan_parquet for lazy loading\n",
    "main_df = pl.scan_parquet(main_path)\n",
    "#hq_df = pl.scan_parquet(hq_path)\n",
    "\n",
    "nace_propagated_path = os.path.join(\"..\", \"data\", \"source_cleaned\", \"data_by_nace_annual_tidy_propagated.parquet\")\n",
    "nace_propagated_df = pl.scan_parquet(nace_propagated_path)\n",
    "\n",
    "macro_indicators_path = os.path.join(\"..\", \"data\", \"source_cleaned\", \"economy_annual_tidy.parquet\")\n",
    "macro_indicators_df = pl.scan_parquet(macro_indicators_path)\n",
    "\n",
    "print(\"Data loaded using lazy evaluation\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "94a54315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Main DataFrame Structure ===\n",
      "Main columns: ['ico', 'year', 'other_liabilities', 'costs', 'sales_revenue', 'equity', 'profit_net', 'turnover', 'current_assets', 'oper_profit', 'total_liabilities', 'total_assets', 'total_liabilities_and_equity', 'profit_pre_tax', 'other_assets', 'fixed_assets', 'name', 'main_nace', 'main_nace_code', 'sub_nace_cz', 'sub_nace_cz_code', 'main_okec', 'main_okec_code', 'sub_okec', 'sub_okec_code', 'esa2010', 'esa95', 'locality', 'region', 'num_employees', 'num_employees_cat', 'turnover_cat', 'audit', 'consolidation', 'currency', 'date_founded', 'date_dissolved', 'status', 'legal_form', 'entity_type', 'year_founded', 'year_dissolved', 'is_dissolved', 'operating_margin_cal', 'net_margin_cal', 'roa_ebit_cal', 'roe_cal', 'equity_ratio_cal', 'cost_ratio_cal', 'asset_turnover_cal', 'labor_productivity_cal', 'effective_tax_rate_cal', 'level1_code', 'level2_code', 'name_czso_en', 'industry_flag']\n",
      "\n",
      "=== NACE Propagated DataFrame Structure ===\n",
      "NACE columns: ['czso_code', 'magnus_nace', 'level', 'name_cs', 'name_en', 'year', 'metric', 'value', 'unit', 'source']\n",
      "NACE sample shape: (5, 10)\n",
      "shape: (5, 10)\n",
      "┌───────────┬─────────────┬───────┬─────────────┬───┬─────────────┬─────────┬─────────────┬────────┐\n",
      "│ czso_code ┆ magnus_nace ┆ level ┆ name_cs     ┆ … ┆ metric      ┆ value   ┆ unit        ┆ source │\n",
      "│ ---       ┆ ---         ┆ ---   ┆ ---         ┆   ┆ ---         ┆ ---     ┆ ---         ┆ ---    │\n",
      "│ str       ┆ str         ┆ i64   ┆ str         ┆   ┆ str         ┆ f64     ┆ str         ┆ str    │\n",
      "╞═══════════╪═════════════╪═══════╪═════════════╪═══╪═════════════╪═════════╪═════════════╪════════╡\n",
      "│ A         ┆ A           ┆ 1     ┆ Zemědělství ┆ … ┆ avg_wages_b ┆ 10456.0 ┆ CZK_avg_gro ┆ CZSO   │\n",
      "│           ┆             ┆       ┆ , lesnictví ┆   ┆ y_nace      ┆         ┆ ss_monthly_ ┆        │\n",
      "│           ┆             ┆       ┆ a rybář…    ┆   ┆             ┆         ┆ per_full…   ┆        │\n",
      "│ B         ┆ B           ┆ 1     ┆ Těžba a     ┆ … ┆ avg_wages_b ┆ 16553.0 ┆ CZK_avg_gro ┆ CZSO   │\n",
      "│           ┆             ┆       ┆ dobývání    ┆   ┆ y_nace      ┆         ┆ ss_monthly_ ┆        │\n",
      "│           ┆             ┆       ┆             ┆   ┆             ┆         ┆ per_full…   ┆        │\n",
      "│ C         ┆ C           ┆ 1     ┆ Zpracovatel ┆ … ┆ avg_wages_b ┆ 12845.0 ┆ CZK_avg_gro ┆ CZSO   │\n",
      "│           ┆             ┆       ┆ ský průmysl ┆   ┆ y_nace      ┆         ┆ ss_monthly_ ┆        │\n",
      "│           ┆             ┆       ┆             ┆   ┆             ┆         ┆ per_full…   ┆        │\n",
      "│ D         ┆ D           ┆ 1     ┆ Výroba a    ┆ … ┆ avg_wages_b ┆ 18468.0 ┆ CZK_avg_gro ┆ CZSO   │\n",
      "│           ┆             ┆       ┆ rozvod      ┆   ┆ y_nace      ┆         ┆ ss_monthly_ ┆        │\n",
      "│           ┆             ┆       ┆ elektřiny,  ┆   ┆             ┆         ┆ per_full…   ┆        │\n",
      "│           ┆             ┆       ┆ ply…        ┆   ┆             ┆         ┆             ┆        │\n",
      "│ E         ┆ E           ┆ 1     ┆ Zásobování  ┆ … ┆ avg_wages_b ┆ 13235.0 ┆ CZK_avg_gro ┆ CZSO   │\n",
      "│           ┆             ┆       ┆ vodou;      ┆   ┆ y_nace      ┆         ┆ ss_monthly_ ┆        │\n",
      "│           ┆             ┆       ┆ činnosti    ┆   ┆             ┆         ┆ per_full…   ┆        │\n",
      "│           ┆             ┆       ┆ sou…        ┆   ┆             ┆         ┆             ┆        │\n",
      "└───────────┴─────────────┴───────┴─────────────┴───┴─────────────┴─────────┴─────────────┴────────┘\n",
      "\n",
      "=== Macro Indicators DataFrame Structure ===\n",
      "Macro columns: ['year', 'metric', 'value']\n",
      "Macro sample shape: (5, 3)\n",
      "shape: (5, 3)\n",
      "┌──────┬──────────────────────┬──────────┐\n",
      "│ year ┆ metric               ┆ value    │\n",
      "│ ---  ┆ ---                  ┆ ---      │\n",
      "│ i64  ┆ str                  ┆ f64      │\n",
      "╞══════╪══════════════════════╪══════════╡\n",
      "│ 2000 ┆ cnb_repo_rate_annual ┆ 5.25     │\n",
      "│ 2001 ┆ cnb_repo_rate_annual ┆ 5.04712  │\n",
      "│ 2002 ┆ cnb_repo_rate_annual ┆ 3.794586 │\n",
      "│ 2003 ┆ cnb_repo_rate_annual ┆ 2.356835 │\n",
      "│ 2004 ┆ cnb_repo_rate_annual ┆ 2.280696 │\n",
      "└──────┴──────────────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "# Explore the structure of each dataset\n",
    "print(\"=== Main DataFrame Structure ===\")\n",
    "main_sample = main_df.limit(0).collect()  # Get structure without data\n",
    "print(f\"Main columns: {main_sample.columns}\")\n",
    "\n",
    "print(\"\\n=== NACE Propagated DataFrame Structure ===\")\n",
    "nace_sample = nace_propagated_df.limit(5).collect()\n",
    "print(f\"NACE columns: {nace_sample.columns}\")\n",
    "print(f\"NACE sample shape: {nace_sample.shape}\")\n",
    "print(nace_sample)\n",
    "\n",
    "print(\"\\n=== Macro Indicators DataFrame Structure ===\")\n",
    "macro_sample = macro_indicators_df.limit(5).collect()\n",
    "print(f\"Macro columns: {macro_sample.columns}\")\n",
    "print(f\"Macro sample shape: {macro_sample.shape}\")\n",
    "print(macro_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a4e82a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Loading NACE Matching Table ===\n",
      "=== Transforming NACE data ===\n",
      "Available NACE metrics: ['ppi_by_nace', 'avg_wages_by_nace', 'no_of_employees_by_nace']\n",
      "\n",
      "--- Processing Level 1 NACE data ---\n",
      "Level 1: Max records per (czso_code, year): 3, Expected metrics: 3\n",
      "Level 1 NACE data transformed: ['czso_code', 'year', 'sector_level1_avg_wages_by_nace', 'sector_level1_no_of_employees_by_nace', 'sector_level1_ppi_by_nace', 'level1_nace_en_name']\n",
      "Level 1 shape: (475, 6)\n",
      "\n",
      "--- Processing Level 2 NACE data ---\n",
      "Level 2: Max records per (czso_code, year): 3, Expected metrics: 3\n",
      "Level 2 NACE data transformed: ['czso_code', 'year', 'sector_level2_ppi_by_nace', 'sector_level2_avg_wages_by_nace', 'sector_level2_no_of_employees_by_nace', 'level2_nace_en_name']\n",
      "Level 2 shape: (2125, 6)\n",
      "\n",
      "NACE data transformed to wide format with level-specific prefixes\n",
      "CRITICAL FIX: Removed name_en from pivot index to ensure unique (czso_code, year) combinations\n"
     ]
    }
   ],
   "source": [
    "# Load NACE matching table for proper level1_code and level2_code mapping\n",
    "print(\"=== Loading NACE Matching Table ===\")\n",
    "nace_matching_path = os.path.join(\"..\", \"data\", \"source_cleaned\", \"t_nace_matching.parquet\")\n",
    "nace_matching_df = pl.scan_parquet(nace_matching_path)\n",
    "\n",
    "# Transform NACE data from long to wide format and add level-specific prefixes\n",
    "print(\"=== Transforming NACE data ===\")\n",
    "\n",
    "# First, let's see what metrics we have in the NACE data\n",
    "nace_metrics = nace_propagated_df.select(\"metric\").unique().collect()\n",
    "print(f\"Available NACE metrics: {nace_metrics['metric'].to_list()}\")\n",
    "\n",
    "# Transform NACE Level 1 data from long to wide format\n",
    "print(\"\\n--- Processing Level 1 NACE data ---\")\n",
    "# Filter for level 1 and collect, then pivot with metrics as columns\n",
    "nace_level1_long = nace_propagated_df.filter(pl.col(\"level\") == 1).collect()\n",
    "\n",
    "# Verify uniqueness of czso_code + year combinations for level 1\n",
    "level1_unique_check = nace_level1_long.group_by([\"czso_code\", \"year\"]).len()\n",
    "max_records_per_combo = level1_unique_check.select(pl.col(\"len\").max()).item()\n",
    "expected_metrics = len(nace_metrics['metric'].to_list())\n",
    "print(f\"Level 1: Max records per (czso_code, year): {max_records_per_combo}, Expected metrics: {expected_metrics}\")\n",
    "\n",
    "# Pivot to wide format with metrics as columns\n",
    "# CRITICAL FIX: Remove name_en from pivot index to prevent duplicates\n",
    "nace_level1 = nace_level1_long.pivot(\n",
    "    index=[\"czso_code\", \"year\"],  # Only unique identifiers\n",
    "    on=\"metric\",\n",
    "    values=\"value\"\n",
    ")\n",
    "\n",
    "# Get the name mapping separately (taking first name for each czso_code, year)\n",
    "nace_level1_names = (nace_level1_long\n",
    "                     .select([\"czso_code\", \"year\", \"name_en\"])\n",
    "                     .unique(subset=[\"czso_code\", \"year\"], keep=\"first\"))\n",
    "\n",
    "# Join names back to the pivoted data\n",
    "nace_level1_with_names = nace_level1.join(\n",
    "    nace_level1_names,\n",
    "    on=[\"czso_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add sector_level1_ prefix to metric columns\n",
    "metric_cols = nace_metrics['metric'].to_list()\n",
    "nace_level1_renamed = nace_level1_with_names.rename({\n",
    "    col: f\"sector_level1_{col}\" for col in metric_cols\n",
    "})\n",
    "\n",
    "# Rename name_en to proper suffix\n",
    "nace_level1_renamed = nace_level1_renamed.rename({\"name_en\": \"level1_nace_en_name\"})\n",
    "\n",
    "print(f\"Level 1 NACE data transformed: {nace_level1_renamed.columns}\")\n",
    "print(f\"Level 1 shape: {nace_level1_renamed.shape}\")\n",
    "\n",
    "# Transform NACE Level 2 data from long to wide format\n",
    "print(\"\\n--- Processing Level 2 NACE data ---\")\n",
    "nace_level2_long = nace_propagated_df.filter(pl.col(\"level\") == 2).collect()\n",
    "\n",
    "# Verify uniqueness of czso_code + year combinations for level 2  \n",
    "level2_unique_check = nace_level2_long.group_by([\"czso_code\", \"year\"]).len()\n",
    "max_records_per_combo = level2_unique_check.select(pl.col(\"len\").max()).item()\n",
    "print(f\"Level 2: Max records per (czso_code, year): {max_records_per_combo}, Expected metrics: {expected_metrics}\")\n",
    "\n",
    "# Pivot to wide format with metrics as columns\n",
    "# CRITICAL FIX: Remove name_en from pivot index to prevent duplicates\n",
    "nace_level2 = nace_level2_long.pivot(\n",
    "    index=[\"czso_code\", \"year\"],  # Only unique identifiers\n",
    "    on=\"metric\",\n",
    "    values=\"value\"\n",
    ")\n",
    "\n",
    "# Get the name mapping separately (taking first name for each czso_code, year)\n",
    "nace_level2_names = (nace_level2_long\n",
    "                     .select([\"czso_code\", \"year\", \"name_en\"])\n",
    "                     .unique(subset=[\"czso_code\", \"year\"], keep=\"first\"))\n",
    "\n",
    "# Join names back to the pivoted data\n",
    "nace_level2_with_names = nace_level2.join(\n",
    "    nace_level2_names,\n",
    "    on=[\"czso_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Add sector_level2_ prefix to metric columns\n",
    "nace_level2_renamed = nace_level2_with_names.rename({\n",
    "    col: f\"sector_level2_{col}\" for col in metric_cols\n",
    "})\n",
    "\n",
    "# Rename name_en to proper suffix\n",
    "nace_level2_renamed = nace_level2_renamed.rename({\"name_en\": \"level2_nace_en_name\"})\n",
    "\n",
    "print(f\"Level 2 NACE data transformed: {nace_level2_renamed.columns}\")\n",
    "print(f\"Level 2 shape: {nace_level2_renamed.shape}\")\n",
    "\n",
    "# Convert back to lazy frames for efficient joining\n",
    "nace_level1_renamed = pl.LazyFrame(nace_level1_renamed)\n",
    "nace_level2_renamed = pl.LazyFrame(nace_level2_renamed)\n",
    "\n",
    "print(\"\\nNACE data transformed to wide format with level-specific prefixes\")\n",
    "print(\"CRITICAL FIX: Removed name_en from pivot index to ensure unique (czso_code, year) combinations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fda04487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VERIFYING NACE DATA FOR JOINS ===\n",
      "Level 1 NACE data shape: (475, 6)\n",
      "Columns: ['czso_code', 'year', 'sector_level1_avg_wages_by_nace', 'sector_level1_no_of_employees_by_nace', 'sector_level1_ppi_by_nace', 'level1_nace_en_name']\n",
      "Test case (czso_code=G, year=2020): 1 rows\n",
      "Sample:\n",
      "shape: (1, 6)\n",
      "┌───────────┬──────┬───────────────────┬───────────────────┬───────────────────┬───────────────────┐\n",
      "│ czso_code ┆ year ┆ sector_level1_avg ┆ sector_level1_no_ ┆ sector_level1_ppi ┆ level1_nace_en_na │\n",
      "│ ---       ┆ ---  ┆ _wages_by_nac…    ┆ of_employees_…    ┆ _by_nace          ┆ me                │\n",
      "│ str       ┆ i64  ┆ ---               ┆ ---               ┆ ---               ┆ ---               │\n",
      "│           ┆      ┆ f64               ┆ f64               ┆ f64               ┆ str               │\n",
      "╞═══════════╪══════╪═══════════════════╪═══════════════════╪═══════════════════╪═══════════════════╡\n",
      "│ G         ┆ 2020 ┆ 33482.0           ┆ 496.8             ┆ null              ┆ Wholesale and     │\n",
      "│           ┆      ┆                   ┆                   ┆                   ┆ retail trade; re… │\n",
      "└───────────┴──────┴───────────────────┴───────────────────┴───────────────────┴───────────────────┘\n",
      "Level 1 unique (czso_code, year) combinations: 475\n",
      "Level 1 total rows: 475\n",
      "✅ Level 1 NACE data is properly unique - good for joining!\n",
      "\n",
      "Level 2 NACE data shape: (2125, 6)\n",
      "Columns: ['czso_code', 'year', 'sector_level2_ppi_by_nace', 'sector_level2_avg_wages_by_nace', 'sector_level2_no_of_employees_by_nace', 'level2_nace_en_name']\n",
      "Level 2 unique (czso_code, year) combinations: 2125\n",
      "Level 2 total rows: 2125\n",
      "✅ Level 2 NACE data is properly unique - good for joining!\n"
     ]
    }
   ],
   "source": [
    "# CRITICAL DIAGNOSTIC: Verify NACE data is properly pivoted before joins\n",
    "print(\"=== VERIFYING NACE DATA FOR JOINS ===\")\n",
    "\n",
    "# Check level 1 data structure\n",
    "if 'nace_level1_renamed' in locals():\n",
    "    level1_sample = nace_level1_renamed.collect()\n",
    "    print(f\"Level 1 NACE data shape: {level1_sample.shape}\")\n",
    "    print(f\"Columns: {level1_sample.columns}\")\n",
    "    \n",
    "    # Check for specific case that was causing problems\n",
    "    test_case = level1_sample.filter(pl.col(\"czso_code\") == \"G\").filter(pl.col(\"year\") == 2020)\n",
    "    print(f\"Test case (czso_code=G, year=2020): {test_case.shape[0]} rows\")\n",
    "    if test_case.shape[0] > 0:\n",
    "        print(\"Sample:\")\n",
    "        print(test_case)\n",
    "    \n",
    "    # Check uniqueness\n",
    "    unique_combos = level1_sample.select([\"czso_code\", \"year\"]).n_unique()\n",
    "    total_rows = level1_sample.height\n",
    "    print(f\"Level 1 unique (czso_code, year) combinations: {unique_combos}\")\n",
    "    print(f\"Level 1 total rows: {total_rows}\")\n",
    "    if unique_combos == total_rows:\n",
    "        print(\"✅ Level 1 NACE data is properly unique - good for joining!\")\n",
    "    else:\n",
    "        print(\"❌ Level 1 NACE data still has duplicates!\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ nace_level1_renamed not available\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check level 2 data structure  \n",
    "if 'nace_level2_renamed' in locals():\n",
    "    level2_sample = nace_level2_renamed.collect()\n",
    "    print(f\"Level 2 NACE data shape: {level2_sample.shape}\")\n",
    "    print(f\"Columns: {level2_sample.columns}\")\n",
    "    \n",
    "    # Check uniqueness\n",
    "    unique_combos = level2_sample.select([\"czso_code\", \"year\"]).n_unique()\n",
    "    total_rows = level2_sample.height\n",
    "    print(f\"Level 2 unique (czso_code, year) combinations: {unique_combos}\")\n",
    "    print(f\"Level 2 total rows: {total_rows}\")\n",
    "    if unique_combos == total_rows:\n",
    "        print(\"✅ Level 2 NACE data is properly unique - good for joining!\")\n",
    "    else:\n",
    "        print(\"❌ Level 2 NACE data still has duplicates!\")\n",
    "else:\n",
    "    print(\"❌ nace_level2_renamed not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "412b5269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Transforming Macro data ===\n",
      "Available macro metrics: ['unemp_rate', 'gdp_nominal_prices', 'CPI_YTYPCT', 'deflator_nominal', 'no_of_employees_ths', 'NLGXQ', 'PDTY', 'ITV_ANNPCT', 'HRS', 'IRL', 'EXCHEB', 'NOOQ', 'hicp_dec', 'UNR', 'PCORE_YTYPCT', 'EXCH', 'fx_czk_eur_annual_avg', 'gdp_2020_base_prices', 'ULCDR', 'hicp_overall_roc', 'import_price_index_ex_energy', 'ULC', 'hicp_pure_energy_roc', 'deflator_base_2020', 'gdp_2020_base_prices_sopr', 'MPEN', 'CPV_ANNPCT', 'FBGSQ', 'cnb_repo_rate_annual', 'RPMGS', 'TTRADE', 'IRS', 'nom_gr_avg_wage_czk', 'hicp_energy_full_roc', 'GGFLMQ', 'KTPV_ANNPCT', 'GAP']\n",
      "Macro data transformed to wide format with mac_ prefix\n",
      "Transformed columns: ['year', 'mac_cnb_repo_rate_annual', 'mac_hicp_dec', 'mac_hicp_overall_roc', 'mac_hicp_pure_energy_roc', 'mac_hicp_energy_full_roc', 'mac_nom_gr_avg_wage_czk', 'mac_no_of_employees_ths', 'mac_gdp_nominal_prices', 'mac_gdp_2020_base_prices', 'mac_gdp_2020_base_prices_sopr', 'mac_deflator_nominal', 'mac_deflator_base_2020', 'mac_unemp_rate', 'mac_fx_czk_eur_annual_avg', 'mac_import_price_index_ex_energy', 'mac_FBGSQ', 'mac_NLGXQ', 'mac_GGFLMQ', 'mac_RPMGS', 'mac_IRS', 'mac_IRL', 'mac_GAP', 'mac_NOOQ', 'mac_PCORE_YTYPCT', 'mac_HRS', 'mac_CPI_YTYPCT', 'mac_UNR', 'mac_EXCH', 'mac_MPEN', 'mac_ULC', 'mac_PDTY', 'mac_ULCDR', 'mac_EXCHEB', 'mac_TTRADE', 'mac_KTPV_ANNPCT', 'mac_CPV_ANNPCT', 'mac_ITV_ANNPCT']\n"
     ]
    }
   ],
   "source": [
    "# Transform macro data from long to wide format and add mac_ prefix\n",
    "print(\"=== Transforming Macro data ===\")\n",
    "\n",
    "# First, let's see what metrics we have in the macro data\n",
    "macro_metrics = macro_indicators_df.select(\"metric\").unique().collect()\n",
    "print(f\"Available macro metrics: {macro_metrics['metric'].to_list()}\")\n",
    "\n",
    "# Transform macro data from long to wide format\n",
    "# For lazy frames, we need to collect first\n",
    "macro_wide = macro_indicators_df.collect().pivot(\n",
    "    index=[\"year\"],\n",
    "    on=\"metric\",  # Updated from 'columns' to 'on'\n",
    "    values=\"value\"\n",
    ")\n",
    "\n",
    "# Add mac_ prefix to metric columns (all columns except year)\n",
    "macro_renamed = macro_wide.rename({\n",
    "    col: f\"mac_{col}\" for col in macro_wide.columns \n",
    "    if col != \"year\"\n",
    "})\n",
    "\n",
    "# Convert back to lazy frame for efficient joining\n",
    "macro_renamed = pl.LazyFrame(macro_renamed)\n",
    "\n",
    "print(\"Macro data transformed to wide format with mac_ prefix\")\n",
    "print(f\"Transformed columns: {macro_renamed.collect_schema().names()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "90b1ca09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Adding firm_ prefix to firm-level columns ===\n",
      "Original main columns count: 56\n",
      "Renaming 53 columns with firm_ prefix\n",
      "Firm columns renamed successfully!\n",
      "=== First Merge: Main + Level 1 NACE data ===\n",
      "Main df has level1_code: True\n",
      "Main df has level2_code: True\n",
      "After Level 1 NACE merge - Shape: (3, 60)\n",
      "Level 1 sector columns: ['sector_level1_avg_wages_by_nace', 'sector_level1_no_of_employees_by_nace', 'sector_level1_ppi_by_nace']\n",
      "=== Second Merge: Adding Level 2 NACE data ===\n",
      "After Level 2 NACE merge - Shape: (3, 64)\n",
      "Level 2 sector columns: ['sector_level2_ppi_by_nace', 'sector_level2_avg_wages_by_nace', 'sector_level2_no_of_employees_by_nace']\n",
      "Null check after NACE merges:\n",
      "  null_year: 0\n",
      "  null_sector_level1_avg_wages_by_nace: 2004\n",
      "  null_sector_level2_ppi_by_nace: 40443\n",
      "NACE merges completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Add firm_ prefix to all columns in main_df (except year and join keys)\n",
    "print(\"=== Adding firm_ prefix to firm-level columns ===\")\n",
    "\n",
    "# Get the columns from main_df that need the firm_ prefix\n",
    "main_cols_sample = main_df.limit(0).collect().columns\n",
    "print(f\"Original main columns count: {len(main_cols_sample)}\")\n",
    "\n",
    "# Create rename mapping for all columns except 'year' and join keys\n",
    "firm_rename_map = {}\n",
    "for col in main_cols_sample:\n",
    "    if col not in ['year', 'level1_code', 'level2_code']:  # Keep join keys unchanged\n",
    "        firm_rename_map[col] = f\"firm_{col}\"\n",
    "\n",
    "print(f\"Renaming {len(firm_rename_map)} columns with firm_ prefix\")\n",
    "\n",
    "# Apply the renaming to main_df\n",
    "main_df_renamed = main_df.rename(firm_rename_map)\n",
    "\n",
    "print(\"Firm columns renamed successfully!\")\n",
    "\n",
    "# Step 1: Merge main_df with Level 1 NACE data\n",
    "print(\"=== First Merge: Main + Level 1 NACE data ===\")\n",
    "\n",
    "# The join strategy:\n",
    "# - main_df has level1_code (like 'A', 'B', 'C', etc.)  \n",
    "# - nace_level1_renamed has czso_code (also like 'A', 'B', 'C', etc. for level 1)\n",
    "# - So we can join directly on level1_code = czso_code\n",
    "\n",
    "# Check if we have the right join keys\n",
    "main_cols_renamed = main_df_renamed.limit(0).collect().columns\n",
    "print(f\"Main df has level1_code: {'level1_code' in main_cols_renamed}\")\n",
    "print(f\"Main df has level2_code: {'level2_code' in main_cols_renamed}\")\n",
    "\n",
    "# Perform the first merge with Level 1 NACE data (left join to keep all main data)\n",
    "merged_step1 = main_df_renamed.join(\n",
    "    nace_level1_renamed,\n",
    "    left_on=[\"level1_code\", \"year\"],\n",
    "    right_on=[\"czso_code\", \"year\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Verify the first merge\n",
    "step1_sample = merged_step1.limit(3).collect()\n",
    "print(f\"After Level 1 NACE merge - Shape: {step1_sample.shape}\")\n",
    "level1_sector_cols = [col for col in step1_sample.columns if col.startswith('sector_level1_')]\n",
    "print(f\"Level 1 sector columns: {level1_sector_cols}\")\n",
    "\n",
    "# Step 2: Merge with Level 2 NACE data  \n",
    "print(\"=== Second Merge: Adding Level 2 NACE data ===\")\n",
    "\n",
    "# For level 2, we need to match level2_code (like '01', '02', etc.) with czso_code\n",
    "# Perform the second merge with Level 2 NACE data (left join to keep all existing data)\n",
    "merged_step2 = merged_step1.join(\n",
    "    nace_level2_renamed,\n",
    "    left_on=[\"level2_code\", \"year\"],\n",
    "    right_on=[\"czso_code\", \"year\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Verify the second merge\n",
    "step2_sample = merged_step2.limit(3).collect()\n",
    "print(f\"After Level 2 NACE merge - Shape: {step2_sample.shape}\")\n",
    "level2_sector_cols = [col for col in step2_sample.columns if col.startswith('sector_level2_')]\n",
    "print(f\"Level 2 sector columns: {level2_sector_cols}\")\n",
    "\n",
    "# Check for any missing joins - count nulls in key sector variables\n",
    "level1_check_col = level1_sector_cols[0] if level1_sector_cols else None\n",
    "level2_check_col = level2_sector_cols[0] if level2_sector_cols else None\n",
    "\n",
    "null_check_cols = [\"year\"]\n",
    "if level1_check_col:\n",
    "    null_check_cols.append(level1_check_col)\n",
    "if level2_check_col:\n",
    "    null_check_cols.append(level2_check_col)\n",
    "\n",
    "null_check = merged_step2.select([\n",
    "    pl.col(col).is_null().sum().alias(f\"null_{col}\") for col in null_check_cols\n",
    "]).collect()\n",
    "print(f\"Null check after NACE merges:\")\n",
    "for col in null_check.columns:\n",
    "    print(f\"  {col}: {null_check[col].item()}\")\n",
    "\n",
    "print(\"NACE merges completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f0a37529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Third Merge: Adding Macro data ===\n",
      "Before macro merge - merged_step2 shape: (663296, 64)\n",
      "Macro data shape: (26, 38)\n",
      "Macro years: 1999 to 2024\n",
      "✓ Macro data has unique years\n",
      "After macro merge - Final merged data Shape: (663296, 101)\n",
      "Total columns: 101\n",
      "❌ ROW EXPANSION DETECTED!\n",
      "   Expected: 663,437 rows\n",
      "   Actual: 663,296 rows\n",
      "   Expansion factor: 1.00x\n",
      "   Max firm-year duplicates: 1\n",
      "\n",
      "Column breakdown:\n",
      "  - Firm columns: 58\n",
      "  - Sector Level 1 columns: 3\n",
      "  - Sector Level 2 columns: 3\n",
      "  - Macro columns: 37\n",
      "\n",
      "Sample sector level 1 columns: ['sector_level1_avg_wages_by_nace', 'sector_level1_no_of_employees_by_nace', 'sector_level1_ppi_by_nace']\n",
      "Sample sector level 2 columns: ['sector_level2_ppi_by_nace', 'sector_level2_avg_wages_by_nace', 'sector_level2_no_of_employees_by_nace']\n",
      "Sample macro columns: ['mac_cnb_repo_rate_annual', 'mac_hicp_dec', 'mac_hicp_overall_roc', 'mac_hicp_pure_energy_roc', 'mac_hicp_energy_full_roc']\n",
      "\n",
      "=== Final Verification ===\n",
      "shape: (1, 6)\n",
      "┌────────────┬────────────┬───────────────┬──────────────────┬──────────────────┬──────────────────┐\n",
      "│ total_rows ┆ null_years ┆ null_firm_ico ┆ null_sector_leve ┆ null_sector_leve ┆ null_mac_cnb_rep │\n",
      "│ ---        ┆ ---        ┆ ---           ┆ l1_avg_wa        ┆ l2_ppi_by        ┆ o_rate_annual    │\n",
      "│ u32        ┆ u32        ┆ u32           ┆ ---              ┆ ---              ┆ ---              │\n",
      "│            ┆            ┆               ┆ u32              ┆ u32              ┆ u32              │\n",
      "╞════════════╪════════════╪═══════════════╪══════════════════╪══════════════════╪══════════════════╡\n",
      "│ 663296     ┆ 0          ┆ 0             ┆ 2004             ┆ 40443            ┆ 0                │\n",
      "└────────────┴────────────┴───────────────┴──────────────────┴──────────────────┴──────────────────┘\n",
      "\n",
      "Duplicate firm-year check: Found 0 duplicate firm-year observations.\n",
      "\n",
      "Merge process completed!\n",
      "Final dataset contains 663,296 rows and 101 columns\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Merge with macro data on year\n",
    "print(\"=== Third Merge: Adding Macro data ===\")\n",
    "\n",
    "# Check shapes before merge\n",
    "print(f\"Before macro merge - merged_step2 shape: {merged_step2.collect().shape}\")\n",
    "\n",
    "# Check macro data shape\n",
    "macro_sample = macro_renamed.collect()\n",
    "print(f\"Macro data shape: {macro_sample.shape}\")\n",
    "print(f\"Macro years: {macro_sample['year'].min()} to {macro_sample['year'].max()}\")\n",
    "\n",
    "# Check for duplicate years in macro data\n",
    "macro_year_counts = macro_sample.select(pl.col(\"year\").value_counts()).unnest(\"year\")\n",
    "duplicate_years = macro_year_counts.filter(pl.col(\"count\") > 1)\n",
    "if duplicate_years.height > 0:\n",
    "    print(f\"❌ PROBLEM: Duplicate years in macro data!\")\n",
    "    print(duplicate_years)\n",
    "else:\n",
    "    print(f\"✓ Macro data has unique years\")\n",
    "\n",
    "# Perform the third merge (left join to keep all existing data)\n",
    "merged_final = merged_step2.join(\n",
    "    macro_renamed,\n",
    "    on=\"year\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Collect the final result for verification\n",
    "final_df = merged_final.collect()\n",
    "print(f\"After macro merge - Final merged data Shape: {final_df.shape}\")\n",
    "print(f\"Total columns: {len(final_df.columns)}\")\n",
    "\n",
    "# Check for unexpected row expansion\n",
    "original_main_count = 663437  # Known from previous analysis\n",
    "if final_df.shape[0] != original_main_count:\n",
    "    print(f\"❌ ROW EXPANSION DETECTED!\")\n",
    "    print(f\"   Expected: {original_main_count:,} rows\")\n",
    "    print(f\"   Actual: {final_df.shape[0]:,} rows\")\n",
    "    print(f\"   Expansion factor: {final_df.shape[0] / original_main_count:.2f}x\")\n",
    "    \n",
    "    # Check for duplicate firm-year combinations that might explain this\n",
    "    if \"firm_ico\" in final_df.columns:\n",
    "        ico_year_counts = final_df.group_by([\"firm_ico\", \"year\"]).len()\n",
    "        max_dupes = ico_year_counts.select(pl.col(\"len\").max()).item()\n",
    "        print(f\"   Max firm-year duplicates: {max_dupes}\")\n",
    "        if max_dupes > 1:\n",
    "            print(\"   Sample duplicated firm-year pairs:\")\n",
    "            dupes = ico_year_counts.filter(pl.col(\"len\") > 1).head(5)\n",
    "            print(dupes)\n",
    "else:\n",
    "    print(f\"✓ No unexpected row expansion - maintaining {original_main_count:,} rows\")\n",
    "\n",
    "# Categorize columns by prefix\n",
    "firm_cols = [col for col in final_df.columns if not col.startswith(('sector_', 'mac_'))]\n",
    "sector_level1_cols = [col for col in final_df.columns if col.startswith('sector_level1_')]\n",
    "sector_level2_cols = [col for col in final_df.columns if col.startswith('sector_level2_')]\n",
    "mac_cols = [col for col in final_df.columns if col.startswith('mac_')]\n",
    "\n",
    "print(f\"\\nColumn breakdown:\")\n",
    "print(f\"  - Firm columns: {len(firm_cols)}\")\n",
    "print(f\"  - Sector Level 1 columns: {len(sector_level1_cols)}\")\n",
    "print(f\"  - Sector Level 2 columns: {len(sector_level2_cols)}\")\n",
    "print(f\"  - Macro columns: {len(mac_cols)}\")\n",
    "\n",
    "print(f\"\\nSample sector level 1 columns: {sector_level1_cols[:5]}\")\n",
    "print(f\"Sample sector level 2 columns: {sector_level2_cols[:5]}\")\n",
    "print(f\"Sample macro columns: {mac_cols[:5]}\")\n",
    "\n",
    "# Final verification - check for missing data in key areas\n",
    "print(f\"\\n=== Final Verification ===\")\n",
    "verification = final_df.select([\n",
    "    pl.len().alias(\"total_rows\"),\n",
    "    pl.col(\"year\").is_null().sum().alias(\"null_years\"),\n",
    "    pl.col(\"firm_ico\").is_null().sum().alias(\"null_firm_ico\") if \"firm_ico\" in final_df.columns else pl.lit(0).alias(\"null_firm_ico\"),\n",
    "    pl.col(sector_level1_cols[0]).is_null().sum().alias(f\"null_{sector_level1_cols[0][:20]}\") if sector_level1_cols else pl.lit(0).alias(\"null_sector_level1_sample\"),\n",
    "    pl.col(sector_level2_cols[0]).is_null().sum().alias(f\"null_{sector_level2_cols[0][:20]}\") if sector_level2_cols else pl.lit(0).alias(\"null_sector_level2_sample\"),\n",
    "    pl.col(mac_cols[0]).is_null().sum().alias(f\"null_{mac_cols[0]}\") if mac_cols else pl.lit(0).alias(\"null_mac_sample\")\n",
    "])\n",
    "\n",
    "print(verification)\n",
    "\n",
    "# Check for duplicate firm-year observations\n",
    "if \"firm_ico\" in final_df.columns:\n",
    "    duplicate_rows = final_df.group_by([\"firm_ico\", \"year\"]).len().filter(pl.col(\"len\") > 1)\n",
    "    num_duplicates = duplicate_rows.height\n",
    "    print(f\"\\nDuplicate firm-year check: Found {num_duplicates} duplicate firm-year observations.\")\n",
    "    if num_duplicates > 0:\n",
    "        print(\"Sample of duplicate firm-year pairs:\")\n",
    "        print(duplicate_rows.head())\n",
    "\n",
    "print(f\"\\nMerge process completed!\")\n",
    "print(f\"Final dataset contains {final_df.shape[0]:,} rows and {final_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b9251c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MINIMAL JOIN TEST ===\n",
      "Testing Level 1 join only...\n",
      "Test main data: 6762 records for level1_code=G, year=2020\n",
      "Test NACE Level 1 data: 1 records for czso_code=G, year=2020\n",
      "NACE Level 1 sample:\n",
      "shape: (1, 4)\n",
      "┌───────────┬──────┬─────────────────────────────────┬─────────────────────────────────┐\n",
      "│ czso_code ┆ year ┆ level1_nace_en_name             ┆ sector_level1_avg_wages_by_nac… │\n",
      "│ ---       ┆ ---  ┆ ---                             ┆ ---                             │\n",
      "│ str       ┆ i64  ┆ str                             ┆ f64                             │\n",
      "╞═══════════╪══════╪═════════════════════════════════╪═════════════════════════════════╡\n",
      "│ G         ┆ 2020 ┆ Wholesale and retail trade; re… ┆ 33482.0                         │\n",
      "└───────────┴──────┴─────────────────────────────────┴─────────────────────────────────┘\n",
      "After join: 6762 records\n",
      "Expected: 6762 records (no duplication)\n",
      "✅ No duplication in Level 1 join\n"
     ]
    }
   ],
   "source": [
    "# MINIMAL JOIN TEST - Find the exact source of duplicates\n",
    "print(\"=== MINIMAL JOIN TEST ===\")\n",
    "\n",
    "# Test just the first join - main_df + level 1 NACE\n",
    "print(\"Testing Level 1 join only...\")\n",
    "\n",
    "# Take a small sample for testing\n",
    "test_main = main_df_renamed.filter(pl.col(\"level1_code\") == \"G\").filter(pl.col(\"year\") == 2020).collect()\n",
    "print(f\"Test main data: {test_main.shape[0]} records for level1_code=G, year=2020\")\n",
    "\n",
    "# Check the NACE level 1 data we're joining with\n",
    "test_nace_l1 = nace_level1_renamed.filter(pl.col(\"czso_code\") == \"G\").filter(pl.col(\"year\") == 2020).collect()\n",
    "print(f\"Test NACE Level 1 data: {test_nace_l1.shape[0]} records for czso_code=G, year=2020\")\n",
    "\n",
    "if test_nace_l1.shape[0] > 0:\n",
    "    print(\"NACE Level 1 sample:\")\n",
    "    print(test_nace_l1.select([\"czso_code\", \"year\", \"level1_nace_en_name\", \"sector_level1_avg_wages_by_nace\"]))\n",
    "\n",
    "# Do the join\n",
    "test_join = pl.LazyFrame(test_main).join(\n",
    "    nace_level1_renamed,\n",
    "    left_on=[\"level1_code\", \"year\"],\n",
    "    right_on=[\"czso_code\", \"year\"], \n",
    "    how=\"left\"\n",
    ").collect()\n",
    "\n",
    "print(f\"After join: {test_join.shape[0]} records\")\n",
    "print(f\"Expected: {test_main.shape[0]} records (no duplication)\")\n",
    "\n",
    "if test_join.shape[0] != test_main.shape[0]:\n",
    "    print(f\"❌ DUPLICATION FOUND: {test_join.shape[0] / test_main.shape[0]:.1f}x expansion\")\n",
    "    \n",
    "    # Check the join keys more carefully\n",
    "    print(\"\\\\nAnalyzing duplication...\")\n",
    "    \n",
    "    # Count unique combinations in both datasets\n",
    "    main_combos = test_main[[\"level1_code\", \"year\"]].n_unique()\n",
    "    nace_combos = test_nace_l1[[\"czso_code\", \"year\"]].n_unique() if test_nace_l1.shape[0] > 0 else 0\n",
    "    \n",
    "    print(f\"Main unique (level1_code, year): {main_combos}\")\n",
    "    print(f\"NACE unique (czso_code, year): {nace_combos}\")\n",
    "    \n",
    "    # Show duplicate analysis\n",
    "    if test_join.shape[0] > 0:\n",
    "        firm_year_dupes = test_join.group_by([\"firm_ico\", \"year\"]).len().sort(\"len\", descending=True)\n",
    "        print(\"\\\\nTop firm-year duplicates:\")\n",
    "        print(firm_year_dupes.head())\n",
    "        \n",
    "        # Show the actual duplicated rows for investigation\n",
    "        if firm_year_dupes.height > 0 and firm_year_dupes[0, \"len\"] > 1:\n",
    "            dup_ico = firm_year_dupes[0, \"firm_ico\"]\n",
    "            dup_year = firm_year_dupes[0, \"year\"]\n",
    "            dup_rows = test_join.filter(pl.col(\"firm_ico\") == dup_ico).filter(pl.col(\"year\") == dup_year)\n",
    "            print(f\"\\\\nDuplicate rows for ico={dup_ico}, year={dup_year}:\")\n",
    "            print(dup_rows.select([\"firm_ico\", \"year\", \"level1_code\", \"level1_nace_en_name\"]))\n",
    "else:\n",
    "    print(\"✅ No duplication in Level 1 join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "251a3412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Saving final merged DataFrame ===\n",
      "Final merged DataFrame saved to ../data/data_ready/merged_panel_imputed.parquet\n"
     ]
    }
   ],
   "source": [
    "# save the final merged DataFrame to Parquet\n",
    "print(\"=== Saving final merged DataFrame ===\")\n",
    "final_df.write_parquet(output_path, compression=\"snappy\")\n",
    "print(f\"Final merged DataFrame saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
