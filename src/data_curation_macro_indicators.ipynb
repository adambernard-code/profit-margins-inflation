{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time-Weighted Annual CNB Repo Rates\n",
    "\n",
    "**Data Source**  \n",
    "A text file listing CNB repo rate decisions. Each decision is valid until the next one begins.\n",
    "\n",
    "**Method**  \n",
    "- **Time Weighting**: Instead of taking a simple average of all rates in a given year, we break each `[start_date, next_start)` interval by calendar year boundaries and compute a time‚Äêweighted average. This ensures that a rate valid for a longer period has a proportionally bigger impact on the annual average.\n",
    "\n",
    "**Justification**  \n",
    "- **Accuracy**: Merely averaging rate values ignores how long each rate was in effect. Time weighting aligns the annual rate with real monetary policy conditions experienced throughout the year.\n",
    "- **Usability**: The resulting annual series can be easily merged with other macro data (HICP, wages, etc.) for subsequent econometric analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-weighted annual CNB repo rates saved to: /Users/adam/Library/Mobile Documents/com~apple~CloudDocs/School/Master's Thesis/Analysis/profit-margins-inflation/data/source_cleaned/cnb_repo_annual.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# Paths\n",
    "script_dir = os.getcwd()  # Jupyter Notebook working directory\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n",
    "input_file = os.path.join(project_root, \"data\", \"source_raw\", \"economy\", \"CNB_repo_sazby.txt\")\n",
    "output_folder = os.path.join(project_root, \"data\", \"source_cleaned\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_file = os.path.join(output_folder, \"cnb_repo_annual.parquet\")\n",
    "\n",
    "# Read data\n",
    "df_repo = pd.read_csv(\n",
    "    input_file,\n",
    "    sep=\"|\",\n",
    "    names=[\"VALID_FROM\", \"CNB_REPO_RATE_IN_PCT\"],\n",
    "    header=None,\n",
    "    dtype={\"VALID_FROM\": str}\n",
    ")\n",
    "df_repo[\"VALID_FROM\"] = pd.to_datetime(df_repo[\"VALID_FROM\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_repo[\"CNB_REPO_RATE_IN_PCT\"] = pd.to_numeric(df_repo[\"CNB_REPO_RATE_IN_PCT\"], errors=\"coerce\")\n",
    "df_repo.dropna(subset=[\"VALID_FROM\"], inplace=True)\n",
    "\n",
    "df_repo.sort_values(\"VALID_FROM\", inplace=True)\n",
    "\n",
    "# Next start date (rates are valid until the day before the next decision)\n",
    "df_repo[\"NEXT_START\"] = df_repo[\"VALID_FROM\"].shift(-1)\n",
    "\n",
    "# Define cutoff for the last row\n",
    "if not df_repo.empty:\n",
    "    last_idx = df_repo.index[-1]\n",
    "    last_year = df_repo.loc[last_idx, \"VALID_FROM\"].year\n",
    "    cutoff_date = pd.to_datetime(f\"{last_year + 1}-12-31\")\n",
    "    df_repo.loc[last_idx, \"NEXT_START\"] = cutoff_date\n",
    "\n",
    "def split_interval_by_year(start_date, end_date):\n",
    "    \"\"\"Break [start_date, end_date) into sub-intervals by calendar year.\"\"\"\n",
    "    end_date = end_date - pd.Timedelta(days=1)\n",
    "    if end_date < start_date:\n",
    "        return []\n",
    "    intervals, current = [], start_date\n",
    "    while current <= end_date:\n",
    "        year_end = pd.to_datetime(f\"{current.year}-12-31\")\n",
    "        local_end = min(year_end, end_date)\n",
    "        delta = (local_end - current).days + 1\n",
    "        intervals.append({\"year\": current.year, \"days_in_interval\": delta})\n",
    "        current = local_end + pd.Timedelta(days=1)\n",
    "    total_days = (end_date - start_date).days + 1\n",
    "    for iv in intervals:\n",
    "        iv[\"total_days_for_rate\"] = total_days\n",
    "    return intervals\n",
    "\n",
    "records = []\n",
    "for _, row in df_repo.iterrows():\n",
    "    intervals = split_interval_by_year(row[\"VALID_FROM\"], row[\"NEXT_START\"])\n",
    "    rate = row[\"CNB_REPO_RATE_IN_PCT\"]\n",
    "    for iv in intervals:\n",
    "        frac = iv[\"days_in_interval\"] / iv[\"total_days_for_rate\"]\n",
    "        records.append({\n",
    "            \"year\": iv[\"year\"],\n",
    "            \"repo_rate\": rate,\n",
    "            \"weighted_rate\": rate * frac\n",
    "        })\n",
    "\n",
    "df_intervals = pd.DataFrame(records)\n",
    "if not df_intervals.empty:\n",
    "    # Sum up weighted rates per year\n",
    "    annual_data = df_intervals.groupby(\"year\", as_index=False).agg(\n",
    "        sum_weighted=(\"weighted_rate\", \"sum\"),\n",
    "        sum_rates=(\"repo_rate\", \"count\")  # Not strictly needed, but can track intervals\n",
    "    )\n",
    "    # The sum of weighted_rate for each year is our time-weighted value,\n",
    "    # because each interval's fraction sums to 1 if the year is fully covered.\n",
    "    # However, to confirm partial coverage, let's also track fraction_of_interval:\n",
    "    # (We'll just treat the sum of weighted_rate as the annual average directly.)\n",
    "    \n",
    "    # Alternatively, we can track fraction_of_interval if needed:\n",
    "    # df_intervals[\"fraction_of_interval\"] = df_intervals[\"weighted_rate\"] / df_intervals[\"repo_rate\"]\n",
    "    # Then group and do the final average. For simplicity, let's proceed with the sum of weighted_rate:\n",
    "    \n",
    "    # Weighted average = sum(weighted_rate)\n",
    "    # => We must ensure each year was fully covered for that to reflect a 100% fraction.\n",
    "    # If partial coverage, the sum will be < the actual expected value.\n",
    "    # For a robust approach, let's recalculate fraction_of_interval in the table:\n",
    "    df_intervals[\"fraction_of_interval\"] = df_intervals[\"weighted_rate\"] / df_intervals[\"repo_rate\"]\n",
    "    annual_fracs = df_intervals.groupby(\"year\")[\"fraction_of_interval\"].sum().reset_index()\n",
    "    annual_merged = annual_data.merge(annual_fracs, on=\"year\", how=\"left\")\n",
    "    annual_merged.rename(columns={\"fraction_of_interval\": \"sum_fraction\"}, inplace=True)\n",
    "    \n",
    "    # Final annual rate = sum of weighted_rate / sum_fraction\n",
    "    # sum_weighted is the sum of (rate * fraction_i)\n",
    "    annual_merged[\"cnb_repo_rate_annual\"] = (\n",
    "        annual_merged[\"sum_weighted\"] / annual_merged[\"sum_fraction\"]\n",
    "    )\n",
    "    \n",
    "    final_annual = annual_merged[[\"year\", \"cnb_repo_rate_annual\"]].sort_values(\"year\")\n",
    "    # drop years pre 2000\n",
    "    final_annual = final_annual[final_annual[\"year\"] >= 2000]\n",
    "    # Save to parquet\n",
    "    final_annual.to_parquet(output_file, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "    print(f\"Time-weighted annual CNB repo rates saved to: {output_file}\")\n",
    "    #display(final_annual.head(25))\n",
    "else:\n",
    "    print(\"No records found after year 2000.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual HICP Data Preparation\n",
    "\n",
    "**Data Source:**  \n",
    "Monthly HICP data (Overall index) with columns:\n",
    "- `DATE` (e.g., \"1996-12-31\")\n",
    "- `TIME PERIOD`\n",
    "- `\"HICP - Overall index (ICP.M.CZ.N.000000.4.ANR)\"`\n",
    "\n",
    "**Method:**  \n",
    "- **Parse Dates & Numeric Conversion:**  \n",
    "  Convert the `DATE` column to datetime and the HICP index column to numeric.\n",
    "- **Filter December Values:**  \n",
    "  Select only records where the month is December. December data represents the end-of-year value, which is often used as the annual measure.\n",
    "- **Extract Year:**  \n",
    "  Create a `year` column from the December dates.\n",
    "- **Output:**  \n",
    "  The resulting DataFrame contains one observation per year with columns `year` and `hicp_dec`. This annual series will serve as the base for merging with other macroeconomic data.\n",
    "\n",
    "**Justification:**  \n",
    "Using December values provides a consistent, end-of-year snapshot. This harmonized annual format simplifies integration with datasets such as the CNB repo rates and firm-level financial data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual HICP (December values) saved to: /Users/adam/Library/Mobile Documents/com~apple~CloudDocs/School/Master's Thesis/Analysis/profit-margins-inflation/data/source_cleaned/hicp_december_annual.parquet\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "script_dir = os.getcwd()  # Current directory in Jupyter\n",
    "project_root = os.path.abspath(os.path.join(script_dir, \"..\"))\n",
    "input_file = os.path.join(project_root, \"data\", \"source_raw\", \"economy\", \"ECB Data Portal_20250402011223_HICP_from1996_CZ.csv\")\n",
    "output_folder = os.path.join(project_root, \"data\", \"source_cleaned\")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "output_file = os.path.join(output_folder, \"hicp_december_annual.parquet\")\n",
    "\n",
    "# Read the HICP data\n",
    "df_hicp = pd.read_csv(input_file)\n",
    "\n",
    "# Convert DATE column to datetime\n",
    "df_hicp[\"DATE\"] = pd.to_datetime(df_hicp[\"DATE\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "\n",
    "# Convert HICP index column to numeric\n",
    "df_hicp[\"hicp_dec\"] = pd.to_numeric(df_hicp[\"HICP - Overall index (ICP.M.CZ.N.000000.4.ANR)\"], errors=\"coerce\")\n",
    "\n",
    "# Filter to keep only December observations\n",
    "df_hicp_dec = df_hicp[df_hicp[\"DATE\"].dt.month == 12].copy()\n",
    "\n",
    "# Extract year from DATE\n",
    "df_hicp_dec[\"year\"] = df_hicp_dec[\"DATE\"].dt.year\n",
    "\n",
    "# Select only the required columns for annual data\n",
    "df_hicp_annual = df_hicp_dec[[\"year\", \"hicp_dec\"]].reset_index(drop=True)\n",
    "\n",
    "# remove pre 2000 data\n",
    "df_hicp_annual = df_hicp_annual[df_hicp_annual[\"year\"] >= 2000]\n",
    "\n",
    "# Save the annual HICP data to a Parquet file\n",
    "df_hicp_annual.to_parquet(output_file, engine=\"pyarrow\", compression=\"snappy\", index=False)\n",
    "\n",
    "print(\"Annual HICP (December values) saved to:\", output_file)\n",
    "#display(df_hicp_annual.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
